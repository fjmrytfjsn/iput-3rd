\documentclass{ltjsarticle}

\title{
    活性化関数・学習率・バッチサイズの違いによる
    CNN分類モデルの学習比較実験レポート
}
\author{藤村勇仁}
\date{\today}

\input{macro}
\begin{document}
\input{titlepage}

\TableOfContents

\section{はじめに}
本実験では、畳み込みニューラルネットワーク（CNN）において、活性化関数（ReLU, sigmoid）、学習率、バッチサイズの違いが学習速度や検証精度に与える影響を比較・検討した。最適なハイパーパラメータを探索することは、モデルの性能向上に不可欠であるため、その基礎的知見を得ることを目的とした。

\section{実験方法}
モデルにはPyTorchを用いたSimpleCNNを採用した。活性化関数としてはReLUおよびsigmoidを中間層に適用し、出力層には活性化関数を用いずクロスエントロピー誤差で損失を算出した。学習率は0.01、0.005、0.0001、0.00005を、バッチサイズは32および64を選択した。

データセットにはKaggleで公開されている「Dogs vs. Cats」データセット\footnote{\url{https://www.kaggle.com/c/dogs-vs-cats}}を利用し、犬と猫の画像を2クラスに分類する課題設定とした。Kaggle上の公式データセットをダウンロードし、PyTorchのDataLoaderを用いて学習・検証データとして読み込んだ。エポック数は5または10とした。

実験に使用したコードは、\ref{sec:code}部に示す。

\section{実験結果}
図\ref{fig:result}に、総学習時間と精度の関係を示す。
\fig[]{result}{実験結果の散布図}{../compare.png}
学習率0.01および0.005では、ReLU・sigmoidのどちらを用いても学習が進まず、エポック5時点の検証精度は0.5前後に留まった。例えば学習率0.01, バッチサイズ32, ReLUの場合、エポック5までの総学習時間は約279.96秒、検証精度は0.5014であった。sigmoidの場合も同様に、総学習時間283.59秒、検証精度0.4986と変わらなかった。学習率0.005でも、ReLUは279.50秒、sigmoidは300.09秒で、いずれも検証精度は0.5014であった。

一方、学習率を0.0001や0.00005まで下げると、ReLUの場合に限り学習が進み、検証精度が向上した。学習率0.0001, ReLU, バッチサイズ32の条件では、5エポックでの総学習時間は296.99秒、検証精度は0.7880となった。sigmoidではこの条件でも精度が0.4976に留まり、依然として学習が進まない傾向が見られた。バッチサイズを64に増やした場合、エポックごとの学習時間は短縮しつつも、検証精度は大きく変化しなかった。

\section{考察}
今回の実験から、学習率が高すぎる場合には損失が減少せず、検証精度も向上しないことが明らかとなった。特にsigmoidを活性化関数に用いた場合、勾配消失などの影響もあり、学習率を下げても十分な精度向上が得られなかった。ReLUでは学習率を適切に下げることで損失が減少し、検証精度も0.78程度まで上昇した。バッチサイズの増加による学習時間短縮効果は確認できたが、今回は精度への顕著な影響は見られなかった。

\section{まとめ}
活性化関数としてReLUを用い、学習率を0.0001程度まで下げることで、CNNモデルは安定して高い精度を発揮できることが確認できた。一方、sigmoidでは学習が進まず、検証精度も上がらない点から、深層学習においてはReLUの有効性が再確認された。今後は層構造や正則化手法、学習率スケジューラの導入なども視野にさらなる性能向上を目指す。

\section{参考データ}
表\ref{tab:result}に各条件でのエポック5時点の総学習時間と検証精度をまとめる。

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
活性化関数 & 学習率 & バッチサイズ & 総学習時間(秒) & 検証精度 \\
\hline
ReLU & 0.01   & 32 & 279.96 & 0.5014 \\
sigmoid & 0.01   & 32 & 283.59 & 0.4986 \\
ReLU & 0.005  & 32 & 279.50 & 0.5014 \\
sigmoid & 0.005  & 32 & 300.09 & 0.5014 \\
ReLU & 0.0001 & 32 & 296.99 & 0.7880 \\
sigmoid & 0.0001 & 32 & 331.55 & 0.4976 \\
\hline
\end{tabular}
\caption{各条件でのエポック5時点の総学習時間と検証精度}
\label{tab:result}
\end{table}

\section{使用コード}\label{sec:code}

\code{dataset}{データセット整形}{../separate.py}

\code{model}{CNNモデル定義（PyTorch）}{../model.py}

\code{trainloop}{学習}{../train_with_time.py}

\code{experiment}{実験の実行}{../experiment.py}

\code{summarygraph}{総学習時間と精度の関係グラフ描画}{../compare_exp.py}

\end{document}

